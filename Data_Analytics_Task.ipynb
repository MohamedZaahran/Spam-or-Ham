{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bkez77PYZRwZ",
    "outputId": "761ad0ef-3a72-4964-b887-eda89dab6c5b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Zahran\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Zahran\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Zahran\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "data = pd.read_csv('data_spam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t1w2WGhweTQ3",
    "outputId": "12dea082-2622-48ff-faa1-88b5effc22c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   v1          5572 non-null   object\n",
      " 1   v2          5572 non-null   object\n",
      " 2   Unnamed: 2  50 non-null     object\n",
      " 3   Unnamed: 3  12 non-null     object\n",
      " 4   Unnamed: 4  6 non-null      object\n",
      "dtypes: object(5)\n",
      "memory usage: 217.8+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "id": "H14C3ZSlazMV"
   },
   "outputs": [],
   "source": [
    "# Dropping the null columns\n",
    "data.drop(columns = ['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis = 1, inplace = True)\n",
    "# Renamining the columns to meaningful names\n",
    "data = data.rename(columns={'v1': 'label', 'v2': 'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "403"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates = data.duplicated()\n",
    "duplicates.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5169 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   label   5169 non-null   object\n",
      " 1   text    5169 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 121.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Dropping the duplicates\n",
    "data = data.drop_duplicates()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "id": "QQsUJX0Ba3LM"
   },
   "outputs": [],
   "source": [
    "# Lowering all the characters in the text column\n",
    "data['text'] = data['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "id": "THyRFdmTa5qR"
   },
   "outputs": [],
   "source": [
    "# Keeping only english letters\n",
    "for index, row in data.iterrows():\n",
    "    row['text'] = re.sub(r\"http\\S+|www\\S+\", \"\", row['text'])\n",
    "    row['text'] = re.sub('[\\W_]+', ' ', row['text'])\n",
    "    row['text'] = re.sub(r'[0-9]', '', row['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "id": "R4x2Y8qla7GN"
   },
   "outputs": [],
   "source": [
    "# Removing the stop words in the text column (the, at, etc...)\n",
    "stop = set(stopwords.words('english'))\n",
    "data['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "id": "iNwfkV0fbC-9"
   },
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "data['tokens'] = data['text'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "id": "iu0vq-TXbGqj"
   },
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "data['lemmatizeList']= ''\n",
    "for index, row in data.iterrows():\n",
    "    lemmatizelist = []\n",
    "    for i in range(len(row['tokens'])):\n",
    "         lemmatizelist.append(lemmatizer.lemmatize(row['tokens'][i]))\n",
    "    row['lemmatizeList'] = lemmatizelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a string column and putting the lemmatized words in the lemmatizeList column in each row separated with a space to be passed to the TF-IDF.\n",
    "data['lemmatizeString'] = ''\n",
    "for index, row in data.iterrows():\n",
    "    lemmatizeString = ''\n",
    "    for i in range(len(row['lemmatizeList'])):\n",
    "        if i == len(row['lemmatizeList']) - 1:\n",
    "            lemmatizeString += row['lemmatizeList'][i]\n",
    "        else:\n",
    "            lemmatizeString += row['lemmatizeList'][i] + \" \"\n",
    "    row['lemmatizeString'] = lemmatizeString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "id": "Sgqjnh3pbbM9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aah</th>\n",
       "      <th>aaniye</th>\n",
       "      <th>aaooooright</th>\n",
       "      <th>aathi</th>\n",
       "      <th>ab</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>abeg</th>\n",
       "      <th>abel</th>\n",
       "      <th>...</th>\n",
       "      <th>zed</th>\n",
       "      <th>zero</th>\n",
       "      <th>zf</th>\n",
       "      <th>zhong</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtorius</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5164</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5165</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5166</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5167</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5168</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5169 rows × 7027 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       aa  aah  aaniye  aaooooright  aathi   ab  abbey  abdomen  abeg  abel  \\\n",
       "0     0.0  0.0     0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
       "1     0.0  0.0     0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
       "2     0.0  0.0     0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
       "3     0.0  0.0     0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
       "4     0.0  0.0     0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
       "...   ...  ...     ...          ...    ...  ...    ...      ...   ...   ...   \n",
       "5164  0.0  0.0     0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
       "5165  0.0  0.0     0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
       "5166  0.0  0.0     0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
       "5167  0.0  0.0     0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
       "5168  0.0  0.0     0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
       "\n",
       "      ...  zed  zero   zf  zhong  zindgi  zoe  zogtorius  zoom  zouk  zyada  \n",
       "0     ...  0.0   0.0  0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0  \n",
       "1     ...  0.0   0.0  0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0  \n",
       "2     ...  0.0   0.0  0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0  \n",
       "3     ...  0.0   0.0  0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0  \n",
       "4     ...  0.0   0.0  0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0  \n",
       "...   ...  ...   ...  ...    ...     ...  ...        ...   ...   ...    ...  \n",
       "5164  ...  0.0   0.0  0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0  \n",
       "5165  ...  0.0   0.0  0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0  \n",
       "5166  ...  0.0   0.0  0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0  \n",
       "5167  ...  0.0   0.0  0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0  \n",
       "5168  ...  0.0   0.0  0.0    0.0     0.0  0.0        0.0   0.0   0.0    0.0  \n",
       "\n",
       "[5169 rows x 7027 columns]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "TfIdf = TfidfVectorizer()\n",
    "x = TfIdf.fit_transform(data['lemmatizeString'])\n",
    "\n",
    "tfidf_tokens = TfIdf.get_feature_names()\n",
    "df_tfidfvect = pd.DataFrame(data = x.toarray(), columns = tfidf_tokens)\n",
    "df_tfidfvect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "id": "vklX2IOKblg7"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "label_map = {'spam': 1, 'ham': 0}\n",
    "\n",
    "\n",
    "# Use map function to apply label_map to label_column\n",
    "data['label'] = data['label'].map(label_map)\n",
    "data['label'] = data['label'].astype(int)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, data['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 563
    },
    "id": "a_Xmtqym4Gqa",
    "outputId": "40d208ec-c000-42f4-8be0-39c187d53a96"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmatizeList</th>\n",
       "      <th>lemmatizeString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n...</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>free entry wkly comp win fa cup final tkts st ...</td>\n",
       "      <td>[free, entry, wkly, comp, win, fa, cup, final,...</td>\n",
       "      <td>[free, entry, wkly, comp, win, fa, cup, final,...</td>\n",
       "      <td>free entry wkly comp win fa cup final tkts st ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>nah think goes usf lives around though</td>\n",
       "      <td>[nah, think, goes, usf, lives, around, though]</td>\n",
       "      <td>[nah, think, go, usf, life, around, though]</td>\n",
       "      <td>nah think go usf life around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text  \\\n",
       "0      0  go jurong point crazy available bugis n great ...   \n",
       "1      0                            ok lar joking wif u oni   \n",
       "2      1  free entry wkly comp win fa cup final tkts st ...   \n",
       "3      0                u dun say early hor u c already say   \n",
       "4      0             nah think goes usf lives around though   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [go, jurong, point, crazy, available, bugis, n...   \n",
       "1                     [ok, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, wkly, comp, win, fa, cup, final,...   \n",
       "3      [u, dun, say, early, hor, u, c, already, say]   \n",
       "4     [nah, think, goes, usf, lives, around, though]   \n",
       "\n",
       "                                       lemmatizeList  \\\n",
       "0  [go, jurong, point, crazy, available, bugis, n...   \n",
       "1                     [ok, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, wkly, comp, win, fa, cup, final,...   \n",
       "3      [u, dun, say, early, hor, u, c, already, say]   \n",
       "4        [nah, think, go, usf, life, around, though]   \n",
       "\n",
       "                                     lemmatizeString  \n",
       "0  go jurong point crazy available bugis n great ...  \n",
       "1                            ok lar joking wif u oni  \n",
       "2  free entry wkly comp win fa cup final tkts st ...  \n",
       "3                u dun say early hor u c already say  \n",
       "4                nah think go usf life around though  "
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataframe after the preprocessing to a csv file.\n",
    "data.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "seSq6ypfbo_L",
    "outputId": "f78408cd-aa20-4dac-d833-8169b43685d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        spam       0.96      1.00      0.98       889\n",
      "         ham       0.99      0.74      0.85       145\n",
      "\n",
      "    accuracy                           0.96      1034\n",
      "   macro avg       0.98      0.87      0.91      1034\n",
      "weighted avg       0.96      0.96      0.96      1034\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Train a Naive Bayes classifier on the training data\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "# Use the classifier to predict the labels for the testing data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the classifier on the testing data\n",
    "target_names = ['spam', 'ham']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X3kf2pRkbsv6",
    "outputId": "ffcd2011-e741-45ae-dc68-c99205a94198"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        spam       0.97      1.00      0.98       889\n",
      "         ham       1.00      0.79      0.88       145\n",
      "\n",
      "    accuracy                           0.97      1034\n",
      "   macro avg       0.98      0.90      0.93      1034\n",
      "weighted avg       0.97      0.97      0.97      1034\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score, confusion_matrix\n",
    "\n",
    "# Train a random forest classifier on the training data\n",
    "clf = RandomForestClassifier().fit(X_train, y_train)\n",
    "\n",
    "# Use the classifier to predict the labels for the testing data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the classifier on the testing data\n",
    "target_names = ['spam', 'ham']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lx04NlJNbwmr",
    "outputId": "1d9833f3-4f65-4f4a-eb62-8093da25a848"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        spam       0.97      1.00      0.98       889\n",
      "         ham       0.98      0.81      0.89       145\n",
      "\n",
      "    accuracy                           0.97      1034\n",
      "   macro avg       0.98      0.91      0.94      1034\n",
      "weighted avg       0.97      0.97      0.97      1034\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine SVM\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score, confusion_matrix\n",
    "\n",
    "# Train an SVM classifier on the training data\n",
    "clf = SVC().fit(X_train, y_train)\n",
    "\n",
    "# Use the classifier to predict the labels for the testing data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the classifier on the testing data\n",
    "target_names = ['spam', 'ham']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_SkVe7jCb-37",
    "outputId": "5963e5e0-2d0f-4b98-c7a7-d56f49f2bd3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        spam       0.97      0.97      0.97       889\n",
      "         ham       0.82      0.82      0.82       145\n",
      "\n",
      "    accuracy                           0.95      1034\n",
      "   macro avg       0.89      0.90      0.89      1034\n",
      "weighted avg       0.95      0.95      0.95      1034\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score, confusion_matrix\n",
    "\n",
    "# Train a decision tree classifier on the training data\n",
    "clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "\n",
    "# Use the classifier to predict the labels for the testing data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the classifier on the testing data\n",
    "target_names = ['spam', 'ham']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
